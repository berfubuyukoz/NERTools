{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NERTools\n",
    "(The code that executes pretrained models of some tools such as StanfordNER, Spacy, and calculates scores such as precision, recall and MCC.)\n",
    "\n",
    "[Github Repo](https://github.com/berfubuyukoz/NERTools)\n",
    "\n",
    "### IMPORTANT NOTE: \n",
    "Default values of arguments assume that the files are on the same folder as this notebook. For more information about the arguments please refer to the README in the GitHub repo. Another version with extension .py is also available to run the code on the command prompt.\n",
    "\n",
    "You can download the files used as default from the Github repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/berfu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import sys\n",
    "from foliaHelper import readFoliaIntoSentences\n",
    "from conllHelper import readConllIntoSentences\n",
    "from metricHelper import findMCC\n",
    "from metricHelper import findPrecisionRecalls\n",
    "from stanfordNER import runStfModel\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default values of arguments. Assumes the files are on the same folder as this notebook. Change the argument values here if you wish them have different values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tool = 'stanford'\n",
    "tag_types = [\"ORG\", \"LOC\", \"PER\"]  # No matter what tag is specified, eval for total data is also calculated.\n",
    "eval_metrics = 0 # 0: Precision and Recall. 1: MCC. 2: Precision, Recall, and MCC\n",
    "annotation_format = 0  # Conll: 0, Folia: 1\n",
    "\n",
    "testfile = './alladjudicated' # For Folia files: './alladjudicated'. For a Conll file: './conll-dataset/test-a.txt'\n",
    "tagger = './stanford-ner.jar' \n",
    "model = './ner-model-english-conll-4class.ser.gz' # Or another model file in your local.\n",
    "\n",
    "#Nevermind the lines below. Paths for my local folder structure.\n",
    "#testfile = './conll-dataset/test-a.txt'\n",
    "#tagger = './stanford-ner-files/stanford-ner.jar'\n",
    "#model = './stanford-ner-files/ner-model-english-conll-4class.ser.gz'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the input data and extracting sentences as token lists and actual tags as a list. Separate parsing methods depending on the annotation format (CONLL, Folia, etc):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function that reads Conll-formatted files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def readConllIntoSentences(testfile):\n",
    "    with open(testfile, 'r') as f:\n",
    "        lines = []\n",
    "        sentences = [[]]\n",
    "        for line in f:\n",
    "            if line != '\\n':\n",
    "                sentences[-1].append(line.split(None, 1)[0])\n",
    "                lines.append(line.split())\n",
    "            else:\n",
    "                sentences.append([])\n",
    "    all_tokens = [line[0] for line in lines]\n",
    "    actual_tags = [line[-1] for line in lines]\n",
    "    actual_tags = ['LOCATION' if re.match('^.*LOC.*$', tag)\n",
    "                       else tag for tag in actual_tags]\n",
    "\n",
    "    actual_tags = ['PERSON' if re.match('^.*PER.*$', tag)\n",
    "                       else tag for tag in actual_tags]\n",
    "\n",
    "    actual_tags = ['ORGANIZATION' if re.match('^.*ORG.*$', tag)\n",
    "                       else tag for tag in actual_tags]\n",
    "\n",
    "    actual_tags = ['O' if re.match('^.*MISC.*$', tag)\n",
    "                       else tag for tag in actual_tags]\n",
    "    return [sentences, all_tokens, actual_tags]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function that reads Folia-formatted files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pynlpl.formats import folia\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "def convertFoliaClass2stfTag(e):\n",
    "    per = 'PERSON'\n",
    "    loc = 'LOCATION'\n",
    "    org = 'ORGANIZATION'\n",
    "    cls = e.cls\n",
    "    if re.match('^.*Target.*$', e.set):\n",
    "        if cls == 'name':\n",
    "            return per\n",
    "    elif re.match('^.*Organizer.*$', e.set):\n",
    "        if cls == 'name':\n",
    "            return org\n",
    "    if cls == 'loc' or cls == 'place' or cls == 'place_pub':\n",
    "        return loc\n",
    "    if cls == 'pname':\n",
    "        return per\n",
    "    if cls == 'fname':\n",
    "        return org\n",
    "    return 'O'\n",
    "\n",
    "\n",
    "def readFoliaIntoSentences(path):\n",
    "    sentences_as_tokens = []\n",
    "    ids = []\n",
    "    id2idx = {}\n",
    "    idx2id = {}\n",
    "    all_tokens = []\n",
    "    actual_stf_tags = []\n",
    "    if os.path.isdir(path):\n",
    "        idx = -1\n",
    "        for filename in os.listdir(path):\n",
    "            doc = folia.Document(file=path + '/' + filename)\n",
    "            for h, sentence in enumerate(doc.sentences()):\n",
    "                sentence_tokenized = sentence.select(folia.Word)\n",
    "                words_folia = list(sentence_tokenized)\n",
    "                sentence_tokens = []\n",
    "                for word in words_folia:\n",
    "                    w_id = word.id\n",
    "                    w_text = word.text()\n",
    "                    if w_id in ids:\n",
    "                        continue\n",
    "                    idx = idx + 1\n",
    "                    if idx == 16307 and w_text == '<P>':\n",
    "                        idx = idx - 1\n",
    "                        continue\n",
    "                    ids.append(w_id)\n",
    "                    id2idx[w_id] = idx\n",
    "                    idx2id[idx] = w_id\n",
    "                    actual_stf_tags.append('O')\n",
    "                    sentence_tokens.append(w_text)\n",
    "                    all_tokens.append(w_text)\n",
    "\n",
    "                sentences_as_tokens.append(sentence_tokens)\n",
    "                for layer in sentence.select(folia.EntitiesLayer):\n",
    "                    for entity in layer.select(folia.Entity):\n",
    "                        for word in entity.wrefs():\n",
    "                            word_id = word.id\n",
    "                            _idx = id2idx[word_id]\n",
    "                            stf_tag = convertFoliaClass2stfTag(entity)\n",
    "                            actual_stf_tags[_idx] = stf_tag\n",
    "\n",
    "    else:\n",
    "        print(\"TODO: Handling of a single Folia file instead of a folder of Folia files.\")\n",
    "    return [sentences_as_tokens, ids, id2idx, idx2id, all_tokens, actual_stf_tags]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code that calls the right file-reader function (one of the two above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "_sentences = []\n",
    "actual_stf_tokens = []\n",
    "actual_stf_tags = []\n",
    "\n",
    "if annotation_format == 0:  # Conll\n",
    "    [_sentences, actual_stf_tokens, actual_stf_tags] = readConllIntoSentences(testfile)\n",
    "elif annotation_format == 1:  # Folia\n",
    "    [_sentences, ids, id2idx, idx2id, actual_stf_tokens, actual_stf_tags] = readFoliaIntoSentences(testfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function that executes the StanfordNER model loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "\n",
    "\n",
    "def runStfModel(sents, tagger, model):\n",
    "    # Prepare NER tagger with english model\n",
    "    ner_tagger = StanfordNERTagger(model, tagger, encoding='utf8')\n",
    "    # Run NER tagger on words\n",
    "    return ner_tagger.tag_sents(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the ner tool desired (stanford, spacy, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ner_tool == 'stanford':\n",
    "    result = runStfModel(_sentences, tagger, model)\n",
    "    token_predTag = [item for sublist in result for item in sublist]\n",
    "else:\n",
    "    print('TODO: Calling other ner tools.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating some intermediate variables for scoring the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_stf_tokens = [tp[0] for tp in token_predTag]\n",
    "pred_stf_tags = [tp[1] for tp in token_predTag]\n",
    "pred_stf_tags = [tp[1] for tp in token_predTag]\n",
    "\n",
    "actual = actual_stf_tags\n",
    "pred = pred_stf_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_act_pred_same = [(i,actual_stf_tokens[i], actual[i],pred[i]) for i in range(len(pred))\n",
    "# if actual[i] == pred[i] and actual[i] != O]\n",
    "\n",
    "# all fp and fn including 'other'\n",
    "idx_token_act_pred_diff = [(i,actual_stf_tokens[i], actual[i],pred[i]) for i in range(len(pred)) if actual[i] != pred[i]]\n",
    "# all fp and fn including 'other'\n",
    "idx_diff = [i[0] for i in idx_token_act_pred_diff]\n",
    "# tp except 'other'\n",
    "idx_tag_numerator = [(i,actual_stf_tokens[i], actual[i], pred[i]) for i in range(len(pred)) if i not in idx_diff and actual[i] != 'O']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function that finds precision and recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findPrecisionRecalls(actual_stf_tokens, actual,pred, idx_diff, tag_types):\n",
    "    tag2scores = {}\n",
    "    # tp for 'loc'\n",
    "    idx_tag_numerator_loc = [(i, actual[i], pred[i]) for i in range(len(pred)) if\n",
    "                             i not in idx_diff and actual[i] == 'LOCATION']\n",
    "    idx_tag_act_loc = [(i, actual[i], pred[i]) for i in range(len(pred)) if actual[i] == 'LOCATION']\n",
    "    idx_tag_pred_loc = [(i, actual[i], pred[i]) for i in range(len(pred)) if pred[i] == 'LOCATION']\n",
    "\n",
    "    \"\"\"EXAMINE RESULTS FOR LOC\"\"\"\n",
    "    actual_locs_missed = [[actual_stf_tokens[a[0]], a] for a in idx_tag_act_loc if pred[a[0]] != 'LOCATION']  # 558, nearly all of them has lower-cased first letters.\n",
    "    actual_locs_catched = [[actual_stf_tokens[a[0]], a] for a in idx_tag_act_loc if pred[a[0]] == 'LOCATION']  # 17. All of them start with upper-cased letters.\n",
    "\n",
    "    # tp for 'per'\n",
    "    idx_tag_numerator_per = [(i, actual[i], pred[i]) for i in range(len(pred)) if\n",
    "                             i not in idx_diff and actual[i] == 'PERSON']\n",
    "    idx_tag_act_per = [(i, actual[i], pred[i]) for i in range(len(pred)) if actual[i] == 'PERSON']\n",
    "    idx_tag_pred_per = [(i, actual[i], pred[i]) for i in range(len(pred)) if pred[i] == 'PERSON']\n",
    "\n",
    "    \"\"\"EXAMINE RESULTS FOR PER\"\"\"\n",
    "    actual_pers_missed = [[actual_stf_tokens[a[0]], a] for a in idx_tag_act_per if\n",
    "                          pred[a[0]] != 'PERSON']  # 558, nearly all of them has lower-cased first letters.\n",
    "    actual_pers_catched = [[actual_stf_tokens[a[0]], a] for a in idx_tag_act_per if\n",
    "                           pred[a[0]] == 'PERSON']  # 17. All of them start with upper-cased letters.\n",
    "\n",
    "    # tp for 'org'\n",
    "    idx_tag_numerator_org = [(i, actual[i], pred[i]) for i in range(len(pred)) if\n",
    "                             i not in idx_diff and actual[i] == 'ORGANIZATION']\n",
    "    idx_tag_act_org = [(i, actual[i], pred[i]) for i in range(len(pred)) if actual[i] == 'ORGANIZATION']\n",
    "    idx_tag_pred_org = [(i, actual[i], pred[i]) for i in range(len(pred)) if pred[i] == 'ORGANIZATION']\n",
    "\n",
    "    \"\"\"EXAMINE RESULTS FOR ORG\"\"\"\n",
    "    actual_orgs_missed = [[actual_stf_tokens[a[0]], a] for a in idx_tag_act_org if\n",
    "                          pred[a[0]] != 'ORGANIZATION']  # 558, nearly all of them has lower-cased first letters.\n",
    "    actual_orgs_catched = [[actual_stf_tokens[a[0]], a] for a in idx_tag_act_org if\n",
    "                           pred[a[0]] == 'ORGANIZATION']  # 17. All of them start with upper-cased letters.\n",
    "\n",
    "    total_numerator = len(idx_tag_numerator_loc) + len(idx_tag_numerator_per) + len(idx_tag_numerator_org)\n",
    "    total_recall = total_numerator / (len(idx_tag_act_loc) + len(idx_tag_act_per) + len(idx_tag_act_org))\n",
    "    total_prec = total_numerator / (len(idx_tag_pred_loc) + len(idx_tag_pred_per) + len(idx_tag_pred_org))\n",
    "    tag2scores['TOTAL'] = [total_prec,total_recall]\n",
    "\n",
    "    if \"LOC\" in tag_types:\n",
    "        loc_recall = len(idx_tag_numerator_loc) / len(idx_tag_act_loc)\n",
    "        loc_prec = len(idx_tag_numerator_loc) / len(idx_tag_pred_loc)\n",
    "        tag2scores['LOC'] = [loc_prec, loc_recall]\n",
    "    if \"PER\" in tag_types:\n",
    "        per_recall = len(idx_tag_numerator_per) / len(idx_tag_act_per)\n",
    "        per_prec = len(idx_tag_numerator_per) / len(idx_tag_pred_per)\n",
    "        tag2scores['PER'] = [per_prec, per_recall]\n",
    "    if \"ORG\" in tag_types:\n",
    "        org_recall = len(idx_tag_numerator_org) / len(idx_tag_act_org)\n",
    "        org_prec = len(idx_tag_numerator_org) / len(idx_tag_pred_org)\n",
    "        tag2scores['ORG'] = [org_prec, org_recall]\n",
    "\n",
    "    return tag2scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function that finds Matthew's Correlation Coefficient (MCC) score. (A good metric for unbalanced data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def findMCC(idx_tag_numerator, idx_act_pred_diff, idx_diff, actual, pred):\n",
    "    # tp for 'loc'\n",
    "    idx_tag_numerator_loc = [(i, actual[i], pred[i]) for i in range(len(pred)) if\n",
    "                             i not in idx_diff and actual[i] == 'LOCATION']\n",
    "    # tp for 'per'\n",
    "    idx_tag_numerator_per = [(i, actual[i], pred[i]) for i in range(len(pred)) if\n",
    "                             i not in idx_diff and actual[i] == 'PERSON']\n",
    "    # tp for 'org'\n",
    "    idx_tag_numerator_org = [(i, actual[i], pred[i]) for i in range(len(pred)) if\n",
    "                             i not in idx_diff and actual[i] == 'ORGANIZATION']\n",
    "\n",
    "    total_tp = idx_tag_numerator\n",
    "    # fp_for loc\n",
    "    # itd[0] corresponds to the 'id' column of the element in the idx_tag_diff list.\n",
    "    fp_loc = [itd[0]\n",
    "              for itd in idx_act_pred_diff if itd[2] == 'LOCATION']\n",
    "\n",
    "    # fn for loc\n",
    "    fn_loc = [itd[0]\n",
    "              for itd in idx_act_pred_diff if itd[1] == 'LOCATION']\n",
    "\n",
    "    # fp_for per\n",
    "    fp_per = [itd[0]\n",
    "              for itd in idx_act_pred_diff if itd[2] == 'PERSON']\n",
    "\n",
    "    # fn for per\n",
    "    fn_per = [itd[0]\n",
    "              for itd in idx_act_pred_diff if itd[1] == 'PERSON']\n",
    "\n",
    "    # fp_for org\n",
    "    fp_org = [itd[0]\n",
    "              for itd in idx_act_pred_diff if itd[2] == 'ORGANIZATION']\n",
    "\n",
    "    # fn for org\n",
    "    fn_org = [itd[0]\n",
    "              for itd in idx_act_pred_diff if itd[1] == 'ORGANIZATION']\n",
    "\n",
    "    # tn for loc\n",
    "    tn_loc = [i for i in range(len(pred)) if i not in idx_diff and actual[i] != 'LOCATION']\n",
    "\n",
    "    # tn for per\n",
    "    tn_per = [i for i in range(len(pred)) if i not in idx_diff and actual[i] != 'PERSON']\n",
    "    # tn for org\n",
    "    tn_org = [i for i in range(len(pred)) if i not in idx_diff and actual[i] != 'ORGANIZATION']\n",
    "\n",
    "    tp_loc = idx_tag_numerator_loc\n",
    "    tp_per = idx_tag_numerator_per\n",
    "    tp_org = idx_tag_numerator_org\n",
    "    total_tp = len(tp_loc) + len(tp_per) + len(tp_org)\n",
    "    total_tn = len(tn_loc) + len(tn_per) + len(tn_org)\n",
    "    total_fp = len(fp_loc) + len(fp_per) + len(fp_org)\n",
    "    total_fn = len(fn_loc) + len(fn_per) + len(fn_org)\n",
    "\n",
    "    total_pred_p = total_tp + total_fp\n",
    "    total_pred_n = total_tn + total_fn\n",
    "    total_actual_n = total_fp + total_tn\n",
    "    total_actual_p = total_tp + total_fn\n",
    "\n",
    "    MCC_numerator = total_tp * total_tn - total_fp * total_fn\n",
    "    MCC_denominator = math.sqrt(total_pred_p * total_pred_n * total_actual_p * total_actual_n)\n",
    "\n",
    "    return MCC_numerator / MCC_denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calcuating scores specified by the user (precision, recall, mcc, etc.), using the functions in the previous two cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate Precision and Recall for tags individually, or MCC, depending on the arguments.\n",
    "if eval_metrics == 0:\n",
    "    tag2precrec = findPrecisionRecalls(actual_stf_tokens, actual,pred, idx_diff, tag_types)\n",
    "elif eval_metrics == 1:\n",
    "    mcc = findMCC(idx_tag_numerator, idx_token_act_pred_diff, idx_diff, actual, pred)\n",
    "elif eval_metrics == 2:\n",
    "    tag2precrec = findPrecisionRecalls(actual_stf_tokens, actual,pred, idx_diff, tag_types)\n",
    "    mcc = findMCC(idx_tag_numerator, idx_token_act_pred_diff, idx_diff, actual, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing scores to the output file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: \n",
      "\n",
      "(Type 'other' results are omitted before calculating scores other than MCC.) \n",
      "\n",
      "TOTAL precision: 0.94\n",
      "\n",
      "TOTAL recall: 0.93\n",
      "\n",
      "\n",
      "LOC precision: 0.95\n",
      "\n",
      "LOC recall: 0.91\n",
      "\n",
      "\n",
      "PER precision: 0.96\n",
      "\n",
      "PER recall: 0.97\n",
      "\n",
      "\n",
      "ORG precision: 0.89\n",
      "\n",
      "ORG recall: 0.9\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Scores: \\n\")\n",
    "print(\"(Type 'other' results are omitted before calculating scores other than MCC.) \\n\")\n",
    "if eval_metrics != 0:\n",
    "    print(\"Matthew's Correlation Coefficient: \"+ str(round(mcc, 2)) + \"\\n\\n\")\n",
    "\n",
    "if eval_metrics != 1:\n",
    "    for t in tag2precrec.keys():\n",
    "        print(t + \" precision: \" + str(round(tag2precrec[t][0], 2)) + \"\\n\")\n",
    "        print(t + \" recall: \" + str(round(tag2precrec[t][1], 2)) + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
